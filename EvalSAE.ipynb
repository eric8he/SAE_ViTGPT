{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1v2K0vSXPzO2Nt-u52Dx3qMr_8G1X7Wx5",
      "authorship_tag": "ABX9TyNqHnXCpM+11GMbS4QeJBWk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric8he/SAE_ViTGPT/blob/main/EvalSAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TYCf1BJkkAq-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sae-lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sae_lens import SAE\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "88DIdBmQkLnE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "BATCH_SIZE = 512  # Reduced batch size to accommodate training\n",
        "NUM_BATCHES_PER_EPOCH = 200"
      ],
      "metadata": {
        "id": "pk_BxQUwkPK5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models and processors\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Initialize SAE\n",
        "sae = SAE.load_from_pretrained(path=\"./\", device=str(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmPcW_dUkPw7",
        "outputId": "2d6631a2-eb09-44cc-b6f8-2aae7044f000"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.46.3\"\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sae_lens/sae.py:145: UserWarning: \n",
            "This SAE has non-empty model_from_pretrained_kwargs. \n",
            "For optimal performance, load the model like so:\n",
            "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainableVisionEncoder(torch.nn.Module):\n",
        "    def __init__(self, model, sae, target_layer):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.sae = sae\n",
        "        self.target_layer = target_layer\n",
        "        self.target_act = None\n",
        "\n",
        "        # Register hook to capture activations\n",
        "        def gather_target_act_hook(mod, inputs, outputs):\n",
        "            self.target_act = inputs[0]\n",
        "            return outputs\n",
        "\n",
        "        self.hook_handle = self.model.decoder.transformer.h[target_layer].register_forward_hook(\n",
        "            gather_target_act_hook\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # Get model outputs\n",
        "        outputs = self.model.generate(pixel_values=pixel_values)\n",
        "\n",
        "        # Get SAE reconstruction\n",
        "        sae_encoded = self.sae.encode(self.target_act.to(torch.float32))\n",
        "        sae_decoded = self.sae.decode(sae_encoded)\n",
        "\n",
        "        return outputs, self.target_act, sae_decoded, sae_encoded\n",
        "\n",
        "    def remove_hook(self):\n",
        "        self.hook_handle.remove()"
      ],
      "metadata": {
        "id": "PdICLS0uqUcV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch_images(images):\n",
        "  return feature_extractor(images=images, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "# Create dataset\n",
        "imgnet = load_dataset(\"imagenet-1k\", split=\"train\", streaming=True)\n",
        "ds = imgnet.shuffle(seed=42)\n",
        "batches = ds.batch(batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "G5hM03j6kRWf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainable model\n",
        "trainable_model = TrainableVisionEncoder(model, sae, 9)\n",
        "trainable_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PJump1mKrZiH",
        "outputId": "ead0b768-e7f1-4418-e8f4-241fe8715e21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainableVisionEncoder(\n",
              "  (model): VisionEncoderDecoderModel(\n",
              "    (encoder): ViTModel(\n",
              "      (embeddings): ViTEmbeddings(\n",
              "        (patch_embeddings): ViTPatchEmbeddings(\n",
              "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (encoder): ViTEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x ViTLayer(\n",
              "            (attention): ViTSdpaAttention(\n",
              "              (attention): ViTSdpaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): ViTSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): ViTIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): ViTOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (pooler): ViTPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (decoder): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2SdpaAttention(\n",
              "              (c_attn): Conv1D(nf=2304, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=768)\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (crossattention): GPT2SdpaAttention(\n",
              "              (c_attn): Conv1D(nf=1536, nx=768)\n",
              "              (q_attn): Conv1D(nf=768, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=768)\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=3072)\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (sae): SAE(\n",
              "    (activation_fn): ReLU()\n",
              "    (hook_sae_input): HookPoint()\n",
              "    (hook_sae_acts_pre): HookPoint()\n",
              "    (hook_sae_acts_post): HookPoint()\n",
              "    (hook_sae_output): HookPoint()\n",
              "    (hook_sae_recons): HookPoint()\n",
              "    (hook_sae_error): HookPoint()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pics = []\n",
        "neurons = {}"
      ],
      "metadata": {
        "id": "jmT9RvBRrcTe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "batch_count = 0\n",
        "with torch.no_grad():\n",
        "  for batch in batches:\n",
        "    if batch_count >= NUM_BATCHES_PER_EPOCH:\n",
        "      break\n",
        "\n",
        "    # Process images\n",
        "    images = [Image.fromarray(np.array(item)) for item in batch[\"image\"]]\n",
        "    images = [i.convert(mode=\"RGB\") if i.mode != \"RGB\" else i for i in images]\n",
        "    batch_pixel_values = process_batch_images(images)\n",
        "\n",
        "    # Forward pass\n",
        "    _, _, _, decoded_vec = trainable_model(batch_pixel_values)\n",
        "\n",
        "    # Store results\n",
        "    for image, acts in zip(images, decoded_vec):\n",
        "      pics.append(image.resize([s // 4 for s in image.size]))\n",
        "      for n_idx in torch.nonzero(acts[0]):\n",
        "        if n_idx.item() not in neurons:\n",
        "          neurons[n_idx.item()] = [len(pics) - 1]\n",
        "        else:\n",
        "          neurons[n_idx.item()].append(len(pics) - 1)\n",
        "\n",
        "    batch_count += 1\n",
        "\n",
        "    print(f\"Batch {batch_count}/{NUM_BATCHES_PER_EPOCH}\")\n",
        "\n",
        "\n",
        "# Clean up\n",
        "trainable_model.remove_hook()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufLOdPgHrd-O",
        "outputId": "bda89f53-54cc-4fdf-def3-14c7f7b386ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1/200\n",
            "Batch 2/200\n",
            "Batch 3/200\n",
            "Batch 4/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 5/200\n",
            "Batch 6/200\n",
            "Batch 7/200\n",
            "Batch 8/200\n",
            "Batch 9/200\n",
            "Batch 10/200\n",
            "Batch 11/200\n",
            "Batch 12/200\n",
            "Batch 13/200\n",
            "Batch 14/200\n",
            "Batch 15/200\n",
            "Batch 16/200\n",
            "Batch 17/200\n",
            "Batch 18/200\n",
            "Batch 19/200\n",
            "Batch 20/200\n",
            "Batch 21/200\n",
            "Batch 22/200\n",
            "Batch 23/200\n",
            "Batch 24/200\n",
            "Batch 25/200\n",
            "Batch 26/200\n",
            "Batch 27/200\n",
            "Batch 28/200\n",
            "Batch 29/200\n",
            "Batch 30/200\n",
            "Batch 31/200\n",
            "Batch 32/200\n",
            "Batch 33/200\n",
            "Batch 34/200\n",
            "Batch 35/200\n",
            "Batch 36/200\n",
            "Batch 37/200\n",
            "Batch 38/200\n",
            "Batch 39/200\n",
            "Batch 40/200\n",
            "Batch 41/200\n",
            "Batch 42/200\n",
            "Batch 43/200\n",
            "Batch 44/200\n",
            "Batch 45/200\n",
            "Batch 46/200\n",
            "Batch 47/200\n",
            "Batch 48/200\n",
            "Batch 49/200\n",
            "Batch 50/200\n",
            "Batch 51/200\n",
            "Batch 52/200\n",
            "Batch 53/200\n",
            "Batch 54/200\n",
            "Batch 55/200\n",
            "Batch 56/200\n",
            "Batch 57/200\n",
            "Batch 58/200\n",
            "Batch 59/200\n",
            "Batch 60/200\n",
            "Batch 61/200\n",
            "Batch 62/200\n",
            "Batch 63/200\n",
            "Batch 64/200\n",
            "Batch 65/200\n",
            "Batch 66/200\n",
            "Batch 67/200\n",
            "Batch 68/200\n",
            "Batch 69/200\n",
            "Batch 70/200\n",
            "Batch 71/200\n",
            "Batch 72/200\n",
            "Batch 73/200\n",
            "Batch 74/200\n",
            "Batch 75/200\n",
            "Batch 76/200\n",
            "Batch 77/200\n",
            "Batch 78/200\n",
            "Batch 79/200\n",
            "Batch 80/200\n",
            "Batch 81/200\n",
            "Batch 82/200\n",
            "Batch 83/200\n",
            "Batch 84/200\n",
            "Batch 85/200\n",
            "Batch 86/200\n",
            "Batch 87/200\n",
            "Batch 88/200\n",
            "Batch 89/200\n",
            "Batch 90/200\n",
            "Batch 91/200\n",
            "Batch 92/200\n",
            "Batch 93/200\n",
            "Batch 94/200\n",
            "Batch 95/200\n",
            "Batch 96/200\n",
            "Batch 97/200\n",
            "Batch 98/200\n",
            "Batch 99/200\n",
            "Batch 100/200\n",
            "Batch 101/200\n",
            "Batch 102/200\n",
            "Batch 103/200\n",
            "Batch 104/200\n",
            "Batch 105/200\n",
            "Batch 106/200\n",
            "Batch 107/200\n",
            "Batch 108/200\n",
            "Batch 109/200\n",
            "Batch 110/200\n",
            "Batch 111/200\n",
            "Batch 112/200\n",
            "Batch 113/200\n",
            "Batch 114/200\n",
            "Batch 115/200\n",
            "Batch 116/200\n",
            "Batch 117/200\n",
            "Batch 118/200\n",
            "Batch 119/200\n",
            "Batch 120/200\n",
            "Batch 121/200\n",
            "Batch 122/200\n",
            "Batch 123/200\n",
            "Batch 124/200\n",
            "Batch 125/200\n",
            "Batch 126/200\n",
            "Batch 127/200\n",
            "Batch 128/200\n",
            "Batch 129/200\n",
            "Batch 130/200\n",
            "Batch 131/200\n",
            "Batch 132/200\n",
            "Batch 133/200\n",
            "Batch 134/200\n",
            "Batch 135/200\n",
            "Batch 136/200\n",
            "Batch 137/200\n",
            "Batch 138/200\n",
            "Batch 139/200\n",
            "Batch 140/200\n",
            "Batch 141/200\n",
            "Batch 142/200\n",
            "Batch 143/200\n",
            "Batch 144/200\n",
            "Batch 145/200\n",
            "Batch 146/200\n",
            "Batch 147/200\n",
            "Batch 148/200\n",
            "Batch 149/200\n",
            "Batch 150/200\n",
            "Batch 151/200\n",
            "Batch 152/200\n",
            "Batch 153/200\n",
            "Batch 154/200\n",
            "Batch 155/200\n",
            "Batch 156/200\n",
            "Batch 157/200\n",
            "Batch 158/200\n",
            "Batch 159/200\n",
            "Batch 160/200\n",
            "Batch 161/200\n",
            "Batch 162/200\n",
            "Batch 163/200\n",
            "Batch 164/200\n",
            "Batch 165/200\n",
            "Batch 166/200\n",
            "Batch 167/200\n",
            "Batch 168/200\n",
            "Batch 169/200\n",
            "Batch 170/200\n",
            "Batch 171/200\n",
            "Batch 172/200\n",
            "Batch 173/200\n",
            "Batch 174/200\n",
            "Batch 175/200\n",
            "Batch 176/200\n",
            "Batch 177/200\n",
            "Batch 178/200\n",
            "Batch 179/200\n",
            "Batch 180/200\n",
            "Batch 181/200\n",
            "Batch 182/200\n",
            "Batch 183/200\n",
            "Batch 184/200\n",
            "Batch 185/200\n",
            "Batch 186/200\n",
            "Batch 187/200\n",
            "Batch 188/200\n",
            "Batch 189/200\n",
            "Batch 190/200\n",
            "Batch 191/200\n",
            "Batch 192/200\n",
            "Batch 193/200\n",
            "Batch 194/200\n",
            "Batch 195/200\n",
            "Batch 196/200\n",
            "Batch 197/200\n",
            "Batch 198/200\n",
            "Batch 199/200\n",
            "Batch 200/200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(output))\n",
        "print(output[0])\n",
        "print(output[0][1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra1bDKworL4L",
        "outputId": "52308f2c-fcdc-46ff-f62a-f6aa5f47828b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102400\n",
            "(<PIL.Image.Image image mode=RGB size=104x125 at 0x7AB0FFAEC4F0>, tensor([[0.0000, 0.0000, 0.6383,  ..., 0.0000, 0.0000, 0.0000]]))\n",
            "torch.Size([1, 24576])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image  # Assuming PIL images are used\n",
        "import torch  # Assuming PyTorch tensors\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_list = output\n",
        "\n",
        "# Separate images and activations\n",
        "images, activations = zip(*input_list)\n",
        "\n",
        "# Concatenate all activation tensors into a single tensor for efficient processing\n",
        "activations = torch.cat(activations, dim=0)  # Shape: [100000, 10]\n",
        "print(activations.shape)\n",
        "\n",
        "# Find sorted indices for each column (activation vector index)\n",
        "sorted_indices = torch.argsort(activations, dim=0, descending=True)\n",
        "print(sorted_indices.shape)\n",
        "\n",
        "# Prepare sorted lists\n",
        "sorted_images = [[images[idx] for idx in sorted_indices[:, i]] for i in range(activations.size(1))]\n",
        "sorted_activations = [activations[sorted_indices[:, i], i] for i in range(activations.size(1))]\n",
        "\n",
        "print(\"done building sorted lists\")\n",
        "\n",
        "# Build the output dictionary, trimming based on the 8th index (index 7 in 0-based indexing)\n",
        "output_dict = {\n",
        "    i: [(sorted_images[i][j], sorted_activations[i][j].item()) for j in range(len(sorted_images[i]))]\n",
        "    for i in range(activations.size(1))\n",
        "    if activations[sorted_indices[7, i], i].item() != 0  # Check 8th index for non-zero\n",
        "}\n",
        "\n",
        "print(f\"Processed {len(input_list)} items into a dictionary with {len(output_dict)} keys.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot-ERzVgGCmi",
        "outputId": "fa845fc7-6cea-4ad9-f1fd-dab29ad0073b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([102400, 24576])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"drive/MyDrive/arr-final.pkl\", \"wb\") as f:\n",
        "    pickle.dump(output, f)"
      ],
      "metadata": {
        "id": "hUcZ34q75cGd"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}
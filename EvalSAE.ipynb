{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1v2K0vSXPzO2Nt-u52Dx3qMr_8G1X7Wx5",
      "authorship_tag": "ABX9TyPNnzV5czk2OIR4hwNveWG+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric8he/SAE_ViTGPT/blob/main/EvalSAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TYCf1BJkkAq-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sae-lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sae_lens import SAE\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "88DIdBmQkLnE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "TRAIN_ALL_LAYERS = False  # Set to True to train the entire model\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 128  # Reduced batch size to accommodate training\n",
        "NUM_EPOCHS = 3\n",
        "TARGET_LAYER = 9\n",
        "NUM_BATCHES_PER_EPOCH = 200"
      ],
      "metadata": {
        "id": "pk_BxQUwkPK5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models and processors\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Initialize SAE\n",
        "sae = SAE.load_from_pretrained(path=\"./\", device=str(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmPcW_dUkPw7",
        "outputId": "080e01d4-17d6-4752-df27-d999a8de75e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.46.3\"\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sae_lens/sae.py:145: UserWarning: \n",
            "This SAE has non-empty model_from_pretrained_kwargs. \n",
            "For optimal performance, load the model like so:\n",
            "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainableVisionEncoder(torch.nn.Module):\n",
        "    def __init__(self, model, sae, target_layer):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.sae = sae\n",
        "        self.target_layer = target_layer\n",
        "        self.target_act = None\n",
        "\n",
        "        # Register hook to capture activations\n",
        "        def gather_target_act_hook(mod, inputs, outputs):\n",
        "            self.target_act = inputs[0]\n",
        "            return outputs\n",
        "\n",
        "        self.hook_handle = self.model.decoder.transformer.h[target_layer].register_forward_hook(\n",
        "            gather_target_act_hook\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # Get model outputs\n",
        "        outputs = self.model.generate(pixel_values=pixel_values)\n",
        "\n",
        "        # Get SAE reconstruction\n",
        "        sae_encoded = self.sae.encode(self.target_act.to(torch.float32))\n",
        "        sae_decoded = self.sae.decode(sae_encoded)\n",
        "\n",
        "        return outputs, self.target_act, sae_decoded, sae_encoded\n",
        "\n",
        "    def remove_hook(self):\n",
        "        self.hook_handle.remove()"
      ],
      "metadata": {
        "id": "PdICLS0uqUcV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch_images(images):\n",
        "  return feature_extractor(images=images, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "# Create dataset\n",
        "imgnet = load_dataset(\"imagenet-1k\", split=\"train\", streaming=True)\n",
        "ds = imgnet.shuffle(seed=42)\n",
        "batches = ds.batch(batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "G5hM03j6kRWf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainable model\n",
        "trainable_model = TrainableVisionEncoder(model, sae, TARGET_LAYER)\n",
        "trainable_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PJump1mKrZiH",
        "outputId": "2e2e7cab-2c39-485d-c707-6b5dbf5679e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainableVisionEncoder(\n",
              "  (model): VisionEncoderDecoderModel(\n",
              "    (encoder): ViTModel(\n",
              "      (embeddings): ViTEmbeddings(\n",
              "        (patch_embeddings): ViTPatchEmbeddings(\n",
              "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (encoder): ViTEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x ViTLayer(\n",
              "            (attention): ViTSdpaAttention(\n",
              "              (attention): ViTSdpaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): ViTSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): ViTIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): ViTOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (pooler): ViTPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (decoder): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2SdpaAttention(\n",
              "              (c_attn): Conv1D(nf=2304, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=768)\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (crossattention): GPT2SdpaAttention(\n",
              "              (c_attn): Conv1D(nf=1536, nx=768)\n",
              "              (q_attn): Conv1D(nf=768, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=768)\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=3072)\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (sae): SAE(\n",
              "    (activation_fn): ReLU()\n",
              "    (hook_sae_input): HookPoint()\n",
              "    (hook_sae_acts_pre): HookPoint()\n",
              "    (hook_sae_acts_post): HookPoint()\n",
              "    (hook_sae_output): HookPoint()\n",
              "    (hook_sae_recons): HookPoint()\n",
              "    (hook_sae_error): HookPoint()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = []"
      ],
      "metadata": {
        "id": "jmT9RvBRrcTe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "batch_count = 0\n",
        "with torch.no_grad():\n",
        "  for batch in batches:\n",
        "    if batch_count >= NUM_BATCHES_PER_EPOCH:\n",
        "      break\n",
        "\n",
        "    # Process images\n",
        "    images = [Image.fromarray(np.array(item)) for item in batch[\"image\"]]\n",
        "    images = [i.convert(mode=\"RGB\") if i.mode != \"RGB\" else i for i in images]\n",
        "    batch_pixel_values = process_batch_images(images)\n",
        "\n",
        "    # Forward pass\n",
        "    _, _, _, decoded_vec = trainable_model(batch_pixel_values)\n",
        "\n",
        "    # Store results\n",
        "    for image, acts in zip(images, decoded_vec):\n",
        "      output.append((image.resize([s // 4 for s in image.size]), acts.cpu()))\n",
        "\n",
        "    batch_count += 1\n",
        "\n",
        "    if batch_count % 10 == 0:\n",
        "      print(f\"Batch {batch_count}/{NUM_BATCHES_PER_EPOCH}\")\n",
        "\n",
        "\n",
        "# Clean up\n",
        "trainable_model.remove_hook()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufLOdPgHrd-O",
        "outputId": "075bbc00-ef8c-4cff-ac5c-3fbe0bedf46d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 10/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 20/200\n",
            "Batch 30/200\n",
            "Batch 40/200\n",
            "Batch 50/200\n",
            "Batch 60/200\n",
            "Batch 70/200\n",
            "Batch 80/200\n",
            "Batch 90/200\n",
            "Batch 100/200\n",
            "Batch 110/200\n",
            "Batch 120/200\n",
            "Batch 130/200\n",
            "Batch 140/200\n",
            "Batch 150/200\n",
            "Batch 160/200\n",
            "Batch 170/200\n",
            "Batch 180/200\n",
            "Batch 190/200\n",
            "Batch 200/200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(output))\n",
        "print(output[0])\n",
        "print(output[0][1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra1bDKworL4L",
        "outputId": "9f49177e-d765-49df-bcf7-a5cdb6509c26"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25600\n",
            "(<PIL.Image.Image image mode=RGB size=104x125 at 0x7FF38A419030>, tensor([[0.0000, 0.0000, 0.6383,  ..., 0.0000, 0.0000, 0.0000]]))\n",
            "torch.Size([1, 24576])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"drive/MyDrive/arr-final.pkl\", \"wb\") as f:\n",
        "    pickle.dump(output, f)"
      ],
      "metadata": {
        "id": "hUcZ34q75cGd"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}